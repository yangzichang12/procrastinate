{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import scipy\n",
    "from datasets import load_dataset\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Workflow:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Speech signal ---> Text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Text ----> Dirichlet Clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Dirichlet Cluster keywords ---> Search Engine / Youtube API\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n"
     ]
    }
   ],
   "source": [
    "### Loading model to device\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/alejandro/Documents/nus-mtechis/procrastinate/procrastinate_data_processor/static/ml_models\n",
      "Parent directory: /home/alejandro/Documents/nus-mtechis/procrastinate/procrastinate_data_processor/static\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "\n",
    "print(\"Current directory:\", current_directory)\n",
    "print(\"Parent directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alejandro/Documents/nus-mtechis/procrastinate/procrastinate_data_processor/static/audio/test_mono_audio.m4a\n"
     ]
    }
   ],
   "source": [
    "sample_audio_path = os.path.join(parent_dir, \"audio\") + \"/test_mono_audio.m4a\"\n",
    "print(sample_audio_path)\n",
    "# audio = scipy.io.wavfile.read(mono_audio_path)[1]\n",
    "# print(audio)\n",
    "# print(audio.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the librosa package here to read mp3 files, for more information regarding the librosa package, you can visit: https://github.com/librosa/librosa\n",
    "Note: Librosa works for mp3, m4a files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -1.3549987e-12\n",
      " -3.1959628e-12 -1.4259451e-12]\n",
      "22050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47535/423069488.py:1: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(sample_audio_path)\n",
      "/home/alejandro/programs/anaconda3/envs/procrastinate/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(sample_audio_path)\n",
    "print(y)\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a sample file for the speech-to-text notebook. This is meant as a test audio to try out whether Whisper works to actually decode the audio into word tokens. Check. Check. One, two, three, four. Zero. Over.\n"
     ]
    }
   ],
   "source": [
    "output_result = pipe(y, generate_kwargs={\"language\": \"english\", \"task\": \"transcribe\"})\n",
    "print(output_result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the transcribed voice into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alejandro/Documents/nus-mtechis/procrastinate/procrastinate_data_processor/static/output/mono_recording_voice_transcribed2.txt\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(parent_dir, 'output') + \"/mono_recording_voice_transcribed2.txt\"\n",
    "print(output_file)\n",
    "transcribed_text = output_result[\"text\"]\n",
    "with open(output_file, \"w\") as dst:\n",
    "    dst.write(transcribed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
